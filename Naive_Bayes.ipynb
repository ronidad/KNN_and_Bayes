{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tj0s0p4DeymK"
      },
      "source": [
        "<font color=\"green\">*To start working on this notebook, or any other notebook that we will use in the Moringa Data Science Course, we will need to save our own copy of it. We can do this by clicking File > Save a Copy in Drive. We will then be able to make edits to our own copy of this notebook.*</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSgQHdZS21iD"
      },
      "source": [
        "# Python Programming: Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2AZSQI69LWk"
      },
      "source": [
        "## Example 1: Gaussian Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Mw9OAPal9LCI"
      },
      "outputs": [],
      "source": [
        "# Example 1\n",
        "# ---\n",
        "# This type of classifier makes the assumption of normal distribution \n",
        "# thus can be best used in cases when all our features are continuous.\n",
        "# ---\n",
        "# Question: Predict the species of flower using 4 different features.\n",
        "# ---\n",
        "# \n",
        "#OUR CODE GOES HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "27ozeIT2BoT4"
      },
      "outputs": [],
      "source": [
        "# Load libraries and datasets to be used in this example\n",
        "#\n",
        "from sklearn import datasets\n",
        "import numpy as np\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "KsdY4zOdB2ky"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Loading our data from python datasets\n",
        "# \n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "TbScEK1FGA0p"
      },
      "outputs": [],
      "source": [
        "# Splitting our data into a training set and a test set\n",
        "# \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=6) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "PV2q7CeTB6P7"
      },
      "outputs": [],
      "source": [
        "# Training our model\n",
        "# \n",
        "clf = GaussianNB()  \n",
        "model = clf.fit(X_train, y_train) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "qfovw-T6CJ0U"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9333333333333333\n"
          ]
        }
      ],
      "source": [
        "# Predicting our test predictors\n",
        "predicted = model.predict(X_test)\n",
        "print(np.mean(predicted == y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "RtGeQcvkB-s6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([1])"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Predicting a new observation\n",
        "new_observation = [[ 10,  3,  4,  0.4]]\n",
        "\n",
        "new_prediction = model.predict(new_observation)\n",
        "new_prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcX1UM0w22nz"
      },
      "source": [
        "## Example 2: Multinomial Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "bczlBDgf2wd6"
      },
      "outputs": [],
      "source": [
        "# Example 2\n",
        "# ---\n",
        "# While working with the multinomial naive bayes classifier, the features are assumed to be multinomially distributed. \n",
        "# This would mean that this type of classifier is commonly used when we have discrete data (e.g. movie ratings 1 and 5).\n",
        "# Let us see how this works.\n",
        "# ----\n",
        "# Question: Build a model to predict whether an sms message is spam or not.\n",
        "# ---\n",
        "# Dataset url = http://bit.ly/SpamCollectionDataset\n",
        "# ---\n",
        "# \n",
        "#OUR CODE GOES HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "SAsqH_uz3h_p"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\ronal\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Importing our libraries \n",
        "\n",
        "# Importing pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Importing numpy\n",
        "import numpy as np\n",
        "\n",
        "# We will also download and import nlkt which is a tokenizer. \n",
        "# This library will help us break (messages) into individual linguistic units i.e. words.\n",
        "#\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "jTtlH2q34L4K"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  label                                            message\n",
              "0   ham  Go until jurong point, crazy.. Available only ...\n",
              "1   ham                      Ok lar... Joking wif u oni...\n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   ham  U dun say so early hor... U c already then say...\n",
              "4   ham  Nah I don't think he goes to usf, he lives aro..."
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Loading and previewing our dataset\n",
        "# \n",
        "df = pd.read_csv('http://bit.ly/SpamCollectionDataset', sep='\\t',  header = None, names = ['label', 'message'])\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "zkCF6JuX4oQM"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-27-751bf0a082bc>:15: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  df['message'] = df.message.str.replace('[^\\w\\s]', '')\n"
          ]
        }
      ],
      "source": [
        "# Pre-processing\n",
        "# We will first emoving useless variance for our task at hand \n",
        "# \n",
        "\n",
        "# Converting the labels from strings to binary values for our classifier\n",
        "# \n",
        "df['label'] = df.label.map({'ham': 0, 'spam': 1})\n",
        "\n",
        "# Converting all characters in the message to lower case\n",
        "# \n",
        "df['message'] = df.message.map(lambda x: x.lower())\n",
        "\n",
        "# Removing any punctuation\n",
        "# \n",
        "df['message'] = df.message.str.replace('[^\\w\\s]', '')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat'"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['message'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5572, 2)"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "-ftbUUxi5UmL"
      },
      "outputs": [],
      "source": [
        "# Pre-processing \n",
        "# Tokenizing the messages into into single words using nltk. \n",
        "\n",
        "# Applying the tokenization\n",
        "# \n",
        "df['message'] = df['message'].apply(nltk.word_tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['go',\n",
              " 'until',\n",
              " 'jurong',\n",
              " 'point',\n",
              " 'crazy',\n",
              " 'available',\n",
              " 'only',\n",
              " 'in',\n",
              " 'bugis',\n",
              " 'n',\n",
              " 'great',\n",
              " 'world',\n",
              " 'la',\n",
              " 'e',\n",
              " 'buffet',\n",
              " 'cine',\n",
              " 'there',\n",
              " 'got',\n",
              " 'amore',\n",
              " 'wat']"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['message'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "NcxdFpHp5xae"
      },
      "outputs": [],
      "source": [
        "# Fifth, we will perform some word stemming. \n",
        "# The idea of stemming is to normalize our text for all variations of words carry the same meaning, \n",
        "# regardless of the tense. One of the most popular stemming algorithms is the Porter Stemmer:\n",
        "# \n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        " \n",
        "df['message'] = df['message'].apply(lambda x: [stemmer.stem(y) for y in x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['go',\n",
              " 'until',\n",
              " 'jurong',\n",
              " 'point',\n",
              " 'crazi',\n",
              " 'avail',\n",
              " 'onli',\n",
              " 'in',\n",
              " 'bugi',\n",
              " 'n',\n",
              " 'great',\n",
              " 'world',\n",
              " 'la',\n",
              " 'e',\n",
              " 'buffet',\n",
              " 'cine',\n",
              " 'there',\n",
              " 'got',\n",
              " 'amor',\n",
              " 'wat']"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['message'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "9ezXwg2e53v6"
      },
      "outputs": [],
      "source": [
        "# Finally, we will transform the data into occurrences, \n",
        "# which will be the features that we will feed into our model\n",
        "# \n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# This converts the list of words into space-separated strings\n",
        "df['message'] = df['message'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "count_vect = CountVectorizer()\n",
        "counts = count_vect.fit_transform(df['message'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<5572x8169 sparse matrix of type '<class 'numpy.float64'>'\n",
              "\twith 72500 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "counts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "mess = 'we are going to test how the stemming works in real world in handy handed handle languages, language'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "we\n",
            "are\n",
            "go\n",
            "to\n",
            "test\n",
            "how\n",
            "the\n",
            "stem\n",
            "work\n",
            "in\n",
            "real\n",
            "world\n",
            "in\n",
            "handi\n",
            "hand\n",
            "handl\n",
            "languag\n",
            ",\n",
            "languag\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "word = word_tokenize(mess)\n",
        "stemmer = PorterStemmer()\n",
        "for w in word:\n",
        "\n",
        "    print(stemmer.stem(w))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "UysRCO4Y6MVI"
      },
      "outputs": [],
      "source": [
        "# We could leave it as the simple word-count per message, but it is better to use Term Frequency Inverse Document Frequency, more known as tf-idf\n",
        "# \n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "transformer = TfidfTransformer().fit(counts)\n",
        "\n",
        "counts = transformer.transform(counts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "2Knhu0766PtJ"
      },
      "outputs": [],
      "source": [
        "# Training the Model\n",
        "# Now that we have performed feature extraction from our data, it is time to build our model. \n",
        "# We will start by splitting our data into training and test sets\n",
        "# \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(counts, df['label'], test_size=0.1, random_state=69)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "qjd6-wy16cmg"
      },
      "outputs": [],
      "source": [
        "# Fitting our model \n",
        "# Then, all that we have to do is initialize the Naive Bayes Classifier and fit the data. \n",
        "# For text classification problems, the Multinomial Naive Bayes Classifier is well-suited\n",
        "# \n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "model = MultinomialNB().fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "GsxzsJuE6mBf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9480286738351255\n"
          ]
        }
      ],
      "source": [
        "# Evaluating the Model\n",
        "# Once we have put together our classifier, we can evaluate its performance in the testing set\n",
        "# \n",
        "predicted = model.predict(X_test)\n",
        "print(np.mean(predicted == y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Expected 2D array, got scalar array instead:\narray=come we test, and test.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-67-88bfeaf5f268>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Enter the word to predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mnew_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'come we test, and test'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_word\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     75\u001b[0m         \"\"\"\n\u001b[0;32m     76\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_X\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m         \u001b[0mjll\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_joint_log_likelihood\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjll\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36m_check_X\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_X\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    610\u001b[0m             \u001b[1;31m# If input is scalar raise error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 612\u001b[1;33m                 raise ValueError(\n\u001b[0m\u001b[0;32m    613\u001b[0m                     \u001b[1;34m\"Expected 2D array, got scalar array instead:\\narray={}.\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m                     \u001b[1;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got scalar array instead:\narray=come we test, and test.\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
          ]
        }
      ],
      "source": [
        "# Enter the word to predict\n",
        "new_word = 'come we test, and test'\n",
        "model.predict(new_word )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0       go until jurong point crazi avail onli in bugi...\n",
              "1                                   ok lar joke wif u oni\n",
              "2       free entri in 2 a wkli comp to win fa cup fina...\n",
              "3             u dun say so earli hor u c alreadi then say\n",
              "4       nah i dont think he goe to usf he live around ...\n",
              "                              ...                        \n",
              "5567    thi is the 2nd time we have tri 2 contact u u ...\n",
              "5568                      will ü b go to esplanad fr home\n",
              "5569         piti wa in mood for that soani other suggest\n",
              "5570    the guy did some bitch but i act like id be in...\n",
              "5571                              rofl it true to it name\n",
              "Name: message, Length: 5572, dtype: object"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['message']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dDIovl6-Bdz"
      },
      "source": [
        "## Example 3: Bernoulli Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bFEsnHL-MAc"
      },
      "outputs": [],
      "source": [
        "# Example 3\n",
        "# ---\n",
        "# Question: It is rare to get a scenario where you have to use the Bernoulli Naive Bayes Classifier. \n",
        "# However, such a case would assume that all our features are binary, \n",
        "# that is they take only two values (e.g. a nominal categorical feature that has been one-hot encoded).\n",
        "# In the following example we will generate a dataset to demonstrate the use of this Classifier.\n",
        "# ---\n",
        "# \n",
        "#OUR CODE GOES HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3IiBTveYHdub"
      },
      "outputs": [],
      "source": [
        "# Importing our libraries\n",
        "# \n",
        "import numpy as np\n",
        "from sklearn.naive_bayes import BernoulliNB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QI14CBvKHk9h"
      },
      "outputs": [],
      "source": [
        "# Creating binary features and target data\n",
        "# \n",
        "# Creating three binary features\n",
        "X = np.random.randint(2, size=(100, 3))\n",
        "\n",
        "# Creating a binary target vector\n",
        "y = np.random.randint(2, size=(100, 1)).ravel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDg6XbQ7HzpW"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1, 1, 0],\n",
              "       [0, 1, 1],\n",
              "       [1, 0, 1],\n",
              "       [0, 1, 0],\n",
              "       [1, 0, 0],\n",
              "       [0, 1, 1],\n",
              "       [0, 1, 0],\n",
              "       [1, 1, 0],\n",
              "       [1, 0, 1],\n",
              "       [0, 1, 0]])"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Viewing first ten observations\n",
        "# \n",
        "X[0:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3D_lyUeH4VB"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.62"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Training our Bernoulli Naive Bayes Classifier\n",
        "# \n",
        "# Creating oour Bernoulli Naive Bayes object with prior probabilities of each class\n",
        "clf = BernoulliNB()\n",
        "\n",
        "# Train model\n",
        "model = clf.fit(X, y)\n",
        "\n",
        "# model score\n",
        "model.score(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oxk9l9YaWgYw"
      },
      "source": [
        "## <font color=\"green\">Challenge 1</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9VOjyqrWpgj"
      },
      "outputs": [],
      "source": [
        "# Challenge 1\n",
        "# ---\n",
        "# Question: Build a model to determine whether a mushroom is edible.\n",
        "# ---\n",
        "# Dataset url = http://bit.ly/MushroomDataset\n",
        "# \n",
        "OUR CODE GOES HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCf-nbGD24qA"
      },
      "source": [
        "## <font color=\"green\">Challenge 2</font> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfhQpHhe286F"
      },
      "outputs": [],
      "source": [
        "# Challenge 2\n",
        "# ---\n",
        "# Question: Given the following two datasets, build a model to determine whether a passenger survived or not.\n",
        "# ---\n",
        "# Train Dataset url = http://bit.ly/TitanicDatasetTrain\n",
        "# Test Dataset url = http://bit.ly/TitanicDatasetTest\n",
        "# ---\n",
        "# \n",
        "OUR CODE GOES HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zlzFxHy2-ky"
      },
      "source": [
        "## <font color=\"green\">Challenge 3</font> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7ExY54F2_m2"
      },
      "outputs": [],
      "source": [
        "# Challenge 3\n",
        "# ---\n",
        "# Question: Build a model to classify a type of glass given the following dataset.\n",
        "# ---\n",
        "# Dataset url = http://bit.ly/GlassDatasetB\n",
        "# Dataset info:\n",
        "# Type of glass: (class) \n",
        "# -) 1 window glass (from vehicle or building) \n",
        "# -) 2 not window glass (containers, tableware, or headlamps)\n",
        "# ---\n",
        "# \n",
        "OUR CODE GOES HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwUCSxpP3AHd"
      },
      "source": [
        "## <font color=\"green\">Challenge 4</font> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Me_kp5ka3BTh"
      },
      "outputs": [],
      "source": [
        "# Challenge 4\n",
        "# ---\n",
        "# Question: Build a classifier to help determine whether future patients do or do not have heart disease.\n",
        "# ---\n",
        "# Dataset url = http://bit.ly/HeartDatasetNB\n",
        "# \n",
        "OUR CODE GOES HERE"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "L2AZSQI69LWk",
        "EcX1UM0w22nz",
        "8dDIovl6-Bdz",
        "Oxk9l9YaWgYw",
        "hCf-nbGD24qA",
        "3zlzFxHy2-ky",
        "JwUCSxpP3AHd"
      ],
      "name": "Python Programming: Naive Bayes",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8e24f623c9d976e65e43b538ecbbc4d478524c94015e92b14b460358aba5245a"
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
